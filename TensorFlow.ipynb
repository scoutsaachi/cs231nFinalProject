{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset_utils import ConstructDataset\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: Dataset object\n",
    "\n",
    "For our own convenience we'll define a lightweight `Dataset` class which lets us iterate over data and labels. This is not the most flexible or most efficient way to iterate through data, but it will serve our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 240, 320, 3) (2, 240, 320)\n"
     ]
    }
   ],
   "source": [
    "train_dset = ConstructDataset()\n",
    "# train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "# val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "# test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 32, 32, 3) (64,)\n",
      "1 (64, 32, 32, 3) (64,)\n",
      "2 (64, 32, 32, 3) (64,)\n",
      "3 (64, 32, 32, 3) (64,)\n",
      "4 (64, 32, 32, 3) (64,)\n",
      "5 (64, 32, 32, 3) (64,)\n",
      "6 (64, 32, 32, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally **use GPU by setting the flag to True below**. It's not neccessary to use a GPU for this assignment; if you are working on Google Cloud then we recommend that you do not use a GPU, as it will be significantly more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Set up some global variables\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Keras Model API\n",
    "Implementing a neural network using the low-level TensorFlow API is a good way to understand how TensorFlow works, but it's a little inconvenient - we had to manually keep track of all Tensors holding learnable parameters, and we had to use a control dependency to implement the gradient descent update step. This was fine for a small network, but could quickly become unweildy for a large complex model.\n",
    "\n",
    "Fortunately TensorFlow provides higher-level packages such as `tf.keras` and `tf.layers` which make it easy to build models out of modular, object-oriented layers; `tf.train` allows you to easily train these models using a variety of different optimization algorithms.\n",
    "\n",
    "In this part of the notebook we will define neural network models using the `tf.keras.Model` API. To implement your own model, you need to do the following:\n",
    "\n",
    "1. Define a new class which subclasses `tf.keras.model`. Give your class an intuitive name that describes it, like `TwoLayerFC` or `ThreeLayerConvNet`.\n",
    "2. In the initializer `__init__()` for your new class, define all the layers you need as class attributes. The `tf.layers` package provides many common neural-network layers, like `tf.layers.Dense` for fully-connected layers and `tf.layers.Conv2D` for convolutional layers. Under the hood, these layers will construct `Variable` Tensors for any learnable parameters. **Warning**: Don't forget to call `super().__init__()` as the first line in your initializer!\n",
    "3. Implement the `call()` method for your class; this implements the forward pass of your model, and defines the *connectivity* of your network. Layers defined in `__init__()` implement `__call__()` so they can be used as function objects that transform input Tensors into output Tensors. Don't define any new layers in `call()`; any layers you want to use in the forward pass should be defined in `__init__()`.\n",
    "\n",
    "After you define your `tf.keras.Model` subclass, you can instantiate it and use it like the model functions from Part II.\n",
    "\n",
    "### Module API: Two-Layer Network\n",
    "\n",
    "Here is a concrete example of using the `tf.keras.Model` API to define a two-layer network. There are a few new bits of API to be aware of here:\n",
    "\n",
    "We use an `Initializer` object to set up the initial values of the learnable parameters of the layers; in particular `tf.variance_scaling_initializer` gives behavior similar to the Kaiming initialization method we used in Part II. You can read more about it here: https://www.tensorflow.org/api_docs/python/tf/variance_scaling_initializer\n",
    "\n",
    "We construct `tf.layers.Dense` objects to represent the two fully-connected layers of the model. In addition to multiplying their input by a weight matrix and adding a bias vector, these layer can also apply a nonlinearity for you. For the first layer we specify a ReLU activation function by passing `activation=tf.nn.relu` to the constructor; the second layer does not apply any activation function.\n",
    "\n",
    "Unfortunately the `flatten` function we defined in Part II is not compatible with the `tf.keras.Model` API; fortunately we can use `tf.layers.flatten` to perform the same operation. The issue with our `flatten` function from Part II has to do with static vs dynamic shapes for Tensors, which is beyond the scope of this notebook; you can read more about the distinction [in the documentation](https://www.tensorflow.org/programmers_guide/faq#tensor_shapes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super().__init__()        \n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.fc1 = tf.layers.Dense(hidden_size, activation=tf.nn.relu,\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.layers.Dense(num_classes,\n",
    "                                   kernel_initializer=initializer)\n",
    "    def call(self, x, training=None):\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "    # As usual in TensorFlow, we first need to define our computational graph.\n",
    "    # To this end we first construct a TwoLayerFC object, then use it to construct\n",
    "    # the scores Tensor.\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, input_size))\n",
    "        scores = model(x)\n",
    "\n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "        \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Three-Layer ConvNet\n",
    "Now it's your turn to implement a three-layer ConvNet using the `tf.keras.Model` API. Your model should have the same architecture used in Part II:\n",
    "\n",
    "1. Convolutional layer with 5 x 5 kernels, with zero-padding of 2\n",
    "2. ReLU nonlinearity\n",
    "3. Convolutional layer with 3 x 3 kernels, with zero-padding of 1\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer to give class scores\n",
    "\n",
    "You should initialize the weights of your network using the same initialization method as was used in the two-layer network above.\n",
    "\n",
    "**Hint**: Refer to the documentation for `tf.layers.Conv2D` and `tf.layers.Dense`:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/layers/Conv2D\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.conv1 = tf.layers.Conv2D(channel_1, [5,5], 1, \"SAME\", activation=tf.nn.relu, \n",
    "                                     kernel_initializer=initializer, use_bias=True)\n",
    "        self.conv2 = tf.layers.Conv2D(channel_2, [3,3], 1, \"SAME\", activation=tf.nn.relu, \n",
    "                                     kernel_initializer=initializer, use_bias=True)\n",
    "        self.fc2 = tf.layers.Dense(num_classes,\n",
    "                                   kernel_initializer=initializer, use_bias=True)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.layers.flatten(x)\n",
    "        scores = self.fc2(x)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the implementation of the `ThreeLayerConvNet` above you can run the following to ensure that your implementation does not crash and produces outputs of the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def test_ThreeLayerConvNet():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Training Loop\n",
    "We need to implement a slightly different training loop when using the `tf.keras.Model` API. Instead of computing gradients and updating the weights of the model manually, we use an `Optimizer` object from the `tf.train` package which takes care of these details for us. You can read more about `Optimizer`s here: https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.device(device):\n",
    "        # Construct the computational graph we will use to train the model. We\n",
    "        # use the model_init_fn to construct the model, declare placeholders for\n",
    "        # the data and labels\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # We need a place holder to explicitly specify if the model is in the training\n",
    "        # phase or not. This is because a number of layers behaves differently in\n",
    "        # training and in testing, e.g., dropout and batch normalization.\n",
    "        # We pass this variable to the computation graph through feed_dict as shown below.\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        # Use the model function to build the forward pass.\n",
    "        scores = model_init_fn(x, is_training)\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # Use the optimizer_fn to construct an Optimizer, then use the optimizer\n",
    "        # to set up the training step. Asking TensorFlow to evaluate the\n",
    "        # train_op returned by optimizer.minimize(loss) will cause us to make a\n",
    "        # single update step using the current minibatch of data.\n",
    "        \n",
    "        # Note that we use tf.control_dependencies to force the model to run\n",
    "        # the tf.GraphKeys.UPDATE_OPS at each training step. tf.GraphKeys.UPDATE_OPS\n",
    "        # holds the operators that update the states of the network.\n",
    "        # For example, the tf.layers.batch_normalization function adds the running mean\n",
    "        # and variance update operators to tf.GraphKeys.UPDATE_OPS.\n",
    "        optimizer = optimizer_init_fn()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Now we can run the computational graph many times to train the model.\n",
    "    # When we call sess.run we ask it to evaluate train_op, which causes the\n",
    "    # model to update.\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Starting epoch %d' % epoch)\n",
    "            for x_np, y_np in train_dset:\n",
    "                feed_dict = {x: x_np, y: y_np, is_training:1}\n",
    "                loss_np, _ = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "                if t % print_every == 0:\n",
    "                    print('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "#                     check_accuracy(sess, val_dset, x, scores, is_training=is_training)\n",
    "                    print()\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Train a Two-Layer Network\n",
    "We can now use the tools defined above to train a two-layer network on CIFAR-10. We define the `model_init_fn` and `optimizer_init_fn` that construct the model and optimizer respectively when called. Here we want to train the model using stochastic gradient descent with no momentum, so we construct a `tf.train.GradientDescentOptimizer` function; you can [read about it here](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer).\n",
    "\n",
    "You don't need to tune any hyperparameters here, but you should achieve accuracies above 40% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.8514\n",
      "\n",
      "Iteration 100, loss = 1.8389\n",
      "\n",
      "Iteration 200, loss = 1.3685\n",
      "\n",
      "Iteration 300, loss = 1.7910\n",
      "\n",
      "Iteration 400, loss = 1.7299\n",
      "\n",
      "Iteration 500, loss = 1.7253\n",
      "\n",
      "Iteration 600, loss = 1.8212\n",
      "\n",
      "Iteration 700, loss = 1.7311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return TwoLayerFC(hidden_size, num_classes)(inputs)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Train a Three-Layer ConvNet\n",
    "Here you should use the tools we've defined above to train a three-layer ConvNet on CIFAR-10. Your ConvNet should use 32 filters in the first convolutional layer and 16 filters in the second layer.\n",
    "\n",
    "To train the model you should use gradient descent with Nesterov momentum 0.9. \n",
    "\n",
    "**HINT**: https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer\n",
    "\n",
    "You don't need to perform any hyperparameter tuning, but you should achieve accuracies above 45% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.5225\n",
      "\n",
      "Iteration 100, loss = 1.7209\n",
      "\n",
      "Iteration 200, loss = 1.3316\n",
      "\n",
      "Iteration 300, loss = 1.4147\n",
      "\n",
      "Iteration 400, loss = 1.1858\n",
      "\n",
      "Iteration 500, loss = 1.4059\n",
      "\n",
      "Iteration 600, loss = 1.4851\n",
      "\n",
      "Iteration 700, loss = 1.2732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model(inputs)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Keras Sequential API\n",
    "In Part III we introduced the `tf.keras.Model` API, which allows you to define models with any number of learnable layers and with arbitrary connectivity between layers.\n",
    "\n",
    "However for many models you don't need such flexibility - a lot of models can be expressed as a sequential stack of layers, with the output of each layer fed to the next layer as input. If your model fits this pattern, then there is an even easier way to define your model: using `tf.keras.Sequential`. You don't need to write any custom classes; you simply call the `tf.keras.Sequential` constructor with a list containing a sequence of layer objects.\n",
    "\n",
    "One complication with `tf.keras.Sequential` is that you must define the shape of the input to the model by passing a value to the `input_shape` of the first layer in your model.\n",
    "\n",
    "### Keras Sequential API: Two-Layer Network\n",
    "Here we rewrite the two-layer fully-connected network using `tf.keras.Sequential`, and train it using the training loop defined above.\n",
    "\n",
    "You don't need to perform any hyperparameter tuning here, but you should see accuracies above 40% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 3.0356\n",
      "Got 131 / 1000 correct (13.10%)\n",
      "\n",
      "Iteration 100, loss = 1.8742\n",
      "Got 385 / 1000 correct (38.50%)\n",
      "\n",
      "Iteration 200, loss = 1.4500\n",
      "Got 412 / 1000 correct (41.20%)\n",
      "\n",
      "Iteration 300, loss = 1.7041\n",
      "Got 404 / 1000 correct (40.40%)\n",
      "\n",
      "Iteration 400, loss = 1.7376\n",
      "Got 431 / 1000 correct (43.10%)\n",
      "\n",
      "Iteration 500, loss = 1.7235\n",
      "Got 445 / 1000 correct (44.50%)\n",
      "\n",
      "Iteration 600, loss = 1.7746\n",
      "Got 440 / 1000 correct (44.00%)\n",
      "\n",
      "Iteration 700, loss = 1.7713\n",
      "Got 442 / 1000 correct (44.20%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    input_shape = (32, 32, 3)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    layers = [\n",
    "        tf.layers.Flatten(input_shape=input_shape),\n",
    "        tf.layers.Dense(hidden_layer_size, activation=tf.nn.relu,\n",
    "                        kernel_initializer=initializer),\n",
    "        tf.layers.Dense(num_classes, kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model(inputs)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Sequential API: Three-Layer ConvNet\n",
    "Here you should use `tf.keras.Sequential` to reimplement the same three-layer ConvNet architecture used in Part II and Part III. As a reminder, your model should have the following architecture:\n",
    "\n",
    "1. Convolutional layer with 16 5x5 kernels, using zero padding of 2\n",
    "2. ReLU nonlinearity\n",
    "3. Convolutional layer with 32 3x3 kernels, using zero padding of 1\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer giving class scores\n",
    "\n",
    "You should initialize the weights of the model using a `tf.variance_scaling_initializer` as above.\n",
    "\n",
    "You should train the model using Nesterov momentum 0.9.\n",
    "\n",
    "You don't need to perform any hyperparameter search, but you should achieve accuracy above 45% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 3.2144\n",
      "Got 82 / 1000 correct (8.20%)\n",
      "\n",
      "Iteration 100, loss = 1.8631\n",
      "Got 384 / 1000 correct (38.40%)\n",
      "\n",
      "Iteration 200, loss = 1.4894\n",
      "Got 406 / 1000 correct (40.60%)\n",
      "\n",
      "Iteration 300, loss = 1.6356\n",
      "Got 440 / 1000 correct (44.00%)\n",
      "\n",
      "Iteration 400, loss = 1.5258\n",
      "Got 476 / 1000 correct (47.60%)\n",
      "\n",
      "Iteration 500, loss = 1.6699\n",
      "Got 487 / 1000 correct (48.70%)\n",
      "\n",
      "Iteration 600, loss = 1.6472\n",
      "Got 487 / 1000 correct (48.70%)\n",
      "\n",
      "Iteration 700, loss = 1.5363\n",
      "Got 510 / 1000 correct (51.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    input_shape = (32, 32, 3)\n",
    "    layers = [\n",
    "        tf.layers.Conv2D(channel_1, [5,5], 1, \"SAME\", activation=tf.nn.relu,\n",
    "                         kernel_initializer=initializer, use_bias=True,\n",
    "                         input_shape=input_shape),\n",
    "        tf.layers.Conv2D(channel_2, [3,3], 1, \"SAME\", activation=tf.nn.relu, \n",
    "                                     kernel_initializer=initializer, use_bias=True),\n",
    "        tf.layers.Flatten(),\n",
    "        tf.layers.Dense(num_classes, kernel_initializer=initializer, use_bias=True)\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return model(inputs)\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: CIFAR-10 open-ended challenge\n",
    "\n",
    "In this section you can experiment with whatever ConvNet architecture you'd like on CIFAR-10.\n",
    "\n",
    "You should experiment with architectures, hyperparameters, loss functions, regularization, or anything else you can think of to train a model that achieves **at least 70%** accuracy on the **validation** set within 10 epochs. You can use the `check_accuracy` and `train` functions from above, or you can implement your own training loop.\n",
    "\n",
    "Describe what you did at the end of the notebook.\n",
    "\n",
    "### Some things you can try:\n",
    "- **Filter size**: Above we used 5x5 and 3x3; is this optimal?\n",
    "- **Number of filters**: Above we used 16 and 32 filters. Would more or fewer do better?\n",
    "- **Pooling**: We didn't use any pooling above. Would this improve the model?\n",
    "- **Normalization**: Would your model be improved with batch normalization, layer normalization, group normalization, or some other normalization strategy?\n",
    "- **Network architecture**: The ConvNet above has only three layers of trainable parameters. Would a deeper model do better?\n",
    "- **Global average pooling**: Instead of flattening after the final convolutional layer, would global average pooling do better? This strategy is used for example in Google's Inception network and in Residual Networks.\n",
    "- **Regularization**: Would some kind of regularization improve performance? Maybe weight decay or dropout?\n",
    "\n",
    "### WARNING: Batch Normalization / Dropout\n",
    "Batch Normalization and Dropout **WILL NOT WORK CORRECTLY** if you use the `train_part34()` function with the object-oriented `tf.keras.Model` or `tf.keras.Sequential` APIs; if you want to use these layers with this training loop then you **must use the tf.layers functional API**.\n",
    "\n",
    "We wrote `train_part34()` to explicitly demonstrate how TensorFlow works; however there are some subtleties that make it tough to handle the object-oriented batch normalization layer in a simple training loop. In practice both `tf.keras` and `tf` provide higher-level APIs which handle the training loop for you, such as [keras.fit](https://keras.io/models/sequential/) and [tf.Estimator](https://www.tensorflow.org/programmers_guide/estimators), both of which will properly handle batch normalization when using the object-oriented API.\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and other hyperparameters. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these, but don't miss the fun if you have time!\n",
    "\n",
    "- Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "  \n",
    "### Have fun and happy training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.4452\n",
      "Got 92 / 1000 correct (9.20%)\n",
      "\n",
      "Iteration 700, loss = 1.5319\n",
      "Got 493 / 1000 correct (49.30%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 1400, loss = 1.3826\n",
      "Got 535 / 1000 correct (53.50%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 2100, loss = 1.1658\n",
      "Got 562 / 1000 correct (56.20%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2800, loss = 1.0562\n",
      "Got 580 / 1000 correct (58.00%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3500, loss = 0.9705\n",
      "Got 594 / 1000 correct (59.40%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 4200, loss = 0.7164\n",
      "Got 596 / 1000 correct (59.60%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4900, loss = 0.7907\n",
      "Got 602 / 1000 correct (60.20%)\n",
      "\n",
      "Starting epoch 7\n",
      "Iteration 5600, loss = 0.7267\n",
      "Got 619 / 1000 correct (61.90%)\n",
      "\n",
      "Starting epoch 8\n",
      "Iteration 6300, loss = 0.9036\n",
      "Got 634 / 1000 correct (63.40%)\n",
      "\n",
      "Starting epoch 9\n",
      "Iteration 7000, loss = 0.6189\n",
      "Got 604 / 1000 correct (60.40%)\n",
      "\n",
      "Starting epoch 10\n",
      "Iteration 7700, loss = 0.5527\n",
      "Got 620 / 1000 correct (62.00%)\n",
      "\n",
      "Iteration 8400, loss = 0.4328\n",
      "Got 612 / 1000 correct (61.20%)\n",
      "\n",
      "Starting epoch 11\n",
      "Iteration 9100, loss = 0.4226\n",
      "Got 596 / 1000 correct (59.60%)\n",
      "\n",
      "Starting epoch 12\n",
      "Iteration 9800, loss = 0.2974\n",
      "Got 611 / 1000 correct (61.10%)\n",
      "\n",
      "Starting epoch 13\n",
      "Iteration 10500, loss = 0.3946\n",
      "Got 593 / 1000 correct (59.30%)\n",
      "\n",
      "Starting epoch 14\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    reg = tf.contrib.layers.l2_regularizer(0.2)\n",
    "    input_shape = (32, 32, 3)\n",
    "    layers = [\n",
    "        tf.layers.Conv2D(channel_1, [7,7], 1, \"SAME\", activation=tf.nn.relu,\n",
    "                         kernel_initializer=initializer, use_bias=True,\n",
    "                         input_shape=input_shape, kernel_regularizer=reg),\n",
    "        tf.layers.Conv2D(channel_2, [5,5], 1, \"SAME\", activation=tf.nn.relu, \n",
    "                                     kernel_initializer=initializer, use_bias=True, \n",
    "                                     kernel_regularizer=reg),\n",
    "        tf.layers.Conv2D(channel_2, [3,3], 1, \"SAME\", activation=tf.nn.relu, \n",
    "                                     kernel_initializer=initializer, use_bias=True,\n",
    "                                     kernel_regularizer=reg),\n",
    "        tf.layers.Flatten(),\n",
    "        tf.layers.Dense(100, kernel_initializer=initializer, use_bias=True,\n",
    "                        activation=tf.nn.relu, kernel_regularizer=reg),\n",
    "        tf.layers.Dense(100, kernel_initializer=initializer, use_bias=True,\n",
    "                        activation=tf.nn.relu, kernel_regularizer=reg),\n",
    "        tf.layers.Dense(num_classes, kernel_initializer=initializer, use_bias=True,\n",
    "                       kernel_regularizer=reg)\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return model(inputs)\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "# device = '/gpu:0'\n",
    "print_every = 700\n",
    "num_epochs = 100\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe what you did \n",
    "\n",
    "In the cell below you should write an explanation of what you did, any additional features that you implemented, and/or any graphs that you made in the process of training and evaluating your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I unfortunately did not have enough time to run this to completion, because I did not have a GPU. I added an extra layer, changed some of the filter sizes, added some dense layers, \n",
    "and added a regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "(60, 120, 120, 3) (60, 14400, 3)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from dataset_utils import getStanfordImagesAndLabels, getWBCImagesAndLabels\n",
    "\n",
    "# _, labels = getStanfordImagesAndLabels(\"test\")\n",
    "images, labels = getWBCImagesAndLabels(\"test\")\n",
    "labels = np.argmax(labels, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m = pickle.load(open(\"results/crfFreezeWBC/predictions.pkl\", \"rb\"))\n",
    "m = np.argmax(m, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8526388888888888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(labels.flatten(), m.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7288116336570267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(labels.flatten(), m.flatten(), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(labels.flatten(), m.flatten(), labels=np.arange(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[9.58084406e-01 1.71365826e-03 4.02019355e-02]\n",
      " [7.45491551e-04 6.71202726e-01 3.28051782e-01]\n",
      " [2.09874886e-01 2.23246872e-01 5.66878242e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAEmCAYAAADmw8JdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xd8FVX6x/HPN4n0XqQkdCyADQRsa28oCNbVtWLXVdSflV1777r2supip1lAwEVdxYKFKipNQUAIiIA0kRqe3x8zwZuQhHvllgl53rzuizsz5848M7l5cs6cmTMyM5xzzsUnK9MBOOdceeJJ0znnEuBJ0znnEuBJ0znnEuBJ0znnEuBJ0znnEuBJcxsl6RZJr4Tvm0v6TVJ2krcxW9JhyVxnHNu8WNLCcH/qb8V6fpPUOpmxZYqkyZIOynQcFYUnzT8pTBgLJVWPmXeepFEZDKtEZvaTmdUws4JMx7I1JG0HPAQcEe7Pkj+7rvDzPyYvuuST1E/SHVsqZ2YdzGxUGkJyeNLcWjnA5Vu7EgX8Z7FljYAqwORMBxIFknIyHUNF5L+oW+d+4GpJdUpaKGlfSWMlLQ//3zdm2ShJd0oaDfwOtA7n3SHp87D5+I6k+pJelbQiXEfLmHU8ImluuGy8pP1LiaOlJJOUI2mfcN2FrzWSZoflsiT1lTRT0hJJAyXVi1nPGZLmhMuuL+vASKoq6cGw/HJJn0mqGi7rGTYpl4X73C7mc7MlXS3pm/BzAyRVkbQjMD0stkzSh7H7Vey4nhe+byvp43A9iyUNiClnktqG72tLeknSojDeGwr/iEnqHcb+gKSlkmZJOqqM/Z4t6Zow/lWSnpfUSNK7klZK+kBS3ZjygyT9HMb4iaQO4fwLgNOAawu/CzHrv07SN8Cq8Ge66TSJpBGSHoxZ/wBJL5T1s3IJMjN//YkXMBs4DHgTuCOcdx4wKnxfD1gKnEFQI/1bOF0/XD4K+AnoEC7fLpw3A2gD1AamAN+H28kBXgL+ExPD6UD9cNlVwM9AlXDZLcAr4fuWgAE5xfahcJt3h9NXAF8CeUBl4Bng9XBZe+A34IBw2UPABuCwUo7PE+G6c4FsYN/wczsCq4DDw+1fG+5zpZjjOgZoGh7DqcBFJe1HSfsVbvO88P3rwPUElYMqwF9iyhnQNnz/EjAEqBmu83vg3HBZb2A9cH64HxcD8wGV8b34kqBWnAv8AkwAOob7/yFwc0z5c8LtVgb+BXwds6wf4Xer2Pq/BpoBVWO/i+H7xuE2DyFIuj8CNTP9+7ItvTIeQHl98UfS3AVYDjSkaNI8AxhT7DNfAL3D96OA24otHwVcHzP9IPBuzPQxsb9UJcS0FNg9fH8LW06aTwHDgaxweipwaMzyJmHCyAFuAvrHLKsOrKOEpBkmqdWFsRRbdiMwsFjZfOCgmON6eszy+4CnS9qPkvaLoknzJeBZIK+EOAxoS5AI1wLtY5ZdGPNz7A3MiFlWLfxs4zK+F6fFTL8BPBUz3Qd4u5TP1gnXXTuc7kfJSfOckr6LMdPHA3OBxcT8ofBXcl7ePN9KZvYdMAzoW2xRU2BOsXlzCGofheaWsMqFMe9XlzBdo3BC0lWSpoZNu2UEtdMG8cQt6ULgIOBUM9sYzm4BvBU2m5cRJNECglpT09h4zWwVUFpHTAOCmt3MEpYVOS7htudS9Lj8HPP+d2L2OUHXAgLGhKcDzikl1koU/VkV/zltisfMfg/flhVTXD9DSdmS7glPh6wgSH6FMZWlpO9NrGEEfwymm9lnWyjrEuRJMzluJmi+xf6izSdIQrGaE9SqCv3pIabC85fXAX8F6ppZHYIar+L87O1ALzNbHrNoLnCUmdWJeVUxs3xgAUGTsHAd1QhODZRkMbCG4DRDcUWOiySF680voeyWrAr/rxYzr3HhGzP72czON7OmBLXHJwvPYxaLdT1Ff1bFf06pcirQi6DFUpug5gx//AxL+35s6XtzJ8EfvCaS/raVMbpiPGkmgZnNAAYAl8XMHgHsKOnU8GT9yQTnBYclabM1Cc4pLgJyJN0E1NrShyQ1C2M908y+L7b4aeBOSS3Csg0l9QqXDQZ6SPqLpErAbZTy/Qlrjy8AD0lqGtao9pFUGRgIdJd0qIJLiK4iaB5/ntDeB9tZRJDcTg+3cQ4xiVrSSZLywsmlBMmmoNg6CsKY7pRUM9z3K4FXEo3nT6hJsO9LCBL/XcWWLwQSupZU0gHA2cCZ4esxSbllf8olwpNm8txGcJ4PAAuuIexBkBSWEDQVe5jZ4iRtbyTwLkGnxRyCmt2Wmm0AhxLUxgbrjx70wkt4HgGGAu9JWknQobFXuD+TgUuA1whqnUuBeWVs52rgW2As8CtwL8G50+kEHViPEdTyjgGOMbN1ce53cecD1xAc4w4UTb5dgK8k/Rbu1+VmNquEdfQhqLX+CHwW7mM6epxfIvjZ5RN0+n1ZbPnzQPvwdMnbW1qZpFrhOi81s/ywaf488J+wRu+SQOGJY+ecc3HwmqZzziXAk6ZzziXAk6ZzziXAk6ZzziVgm7/hXzlVTZVqZjqMyOrYrnmmQ4i0jd5PukVfTxy/2MwaJmt92bVamG1YHVdZW71opJl1S9a247HtJ81KNam8018zHUZkjf7q8UyHEGlr1pXr0fTSom71nOJ3vm0V27A67t/ZNV8/EdcdcMm0zSdN51x5I4jwSImeNJ1z0SIgK6kPGUgqT5rOueiJ8A1MnjSdcxHjzXPnnEuM1zSdcy5OwmuazjkXP3lN0znnEuK95845Fy/vCHLOufgJb54751xCvKbpnHPx8ua5c87FT0C2dwQ551z8/Jymc87Fy5vnzjmXGK9pOudcArym6ZxzcZLfRumcc4nx2yidcy5e3hHknHOJ8ea5c87FycfTdM65RHjz3DnnEuMdQc45lwA/p+mcc3GSN8+dcy4xEa5pRjedlzOH79uOSW/dyHdDbubqsw/fbHnzJnUZ8XQfxgz4ByP/fTm529fZtKxZ47q88+QlTHzjBia8cT3Nm9RLZ+hp897I/7Jbh53osHNb7r/vns2Wr127ltNPPZkOO7dl/333Ys7s2ZuW3X/v3XTYuS27ddiJ998bmcao0+eD9/5Llz3a02nXnXj4gXs3W7527VrOOfNvdNp1Jw47cB9+mjO7yPK5c38ib/vaPPavB9MUcepIiuuVCZ40kyArS/yr71/pdemTdDzhDk7qtic7t25cpMzd/3ccrw4fQ9eT7+auZ9/ltj49Ny177vYzefjF/9HxhDvY//T7WbR0Zbp3IeUKCgq44rJLGPLOu0z8ZgqD+r/O1ClTipTp98Lz1K1Tl8nTZtDn8v/j+n9eB8DUKVMYNKA/EyZNZuiw/3J5n79TUFCQid1ImYKCAq658jIGvTWML8d/yxuDBjBtatHj8/KLL1C7Tl0mfDudiy+9gltu/EeR5ddfdxWHHdEtnWGnRPC0C0+a27Quu7Rk5tzFzM5fwvoNBQwaOYEeB+1WpMzOrZsw6qvpAHw89nt6HLRrOL8xOdlZfPjVNABWrV7H6jXr07sDaTB2zBjatGlLq9atqVSpEiedfArD3hlSpMywd4Zw2hlnAXD8CScy6sP/YWYMe2cIJ518CpUrV6Zlq1a0adOWsWPGZGI3Umb8uDG0bt2Glq2C43P8iX9lxLChRcq8O2wofzvtDAB6HXcCH4/6EDMDYPg7Q2jRshU7t2uf9tiTTkJZ8b0ywZNmEjTdvjbzFi7dNJ2/cCm5DWsXKfPt9/kce+geAPQ6ZHdq1ahKvdrV2aH59ixbuZr+D5zHF69fx11XHEtWhr4MqTR/fj55ec02Tefm5pGfn795mWZBmZycHGrVrs2SJUvIz9/8s/PnF/1sebdg/nxyY/axaW4eCxbML1JmfkyZnJwcatWqza9LlrBq1Soeeeg+rvvnTWmNOZUqbE1TUktJ35Uwf5Skzn9ifb0lPZ6c6JJHbP7Ds2LT/3j4Lfbfsy1fvH4d++/ZlvyFS9lQUEBOThb7dWxD34ff4i+n30+rvAac0XPv9ASeRoU1oljFv/Sllonjs+VdPMdn829VUOaeO27h4kuvoEaNGqkJLgOSmTQldZM0XdIMSX1LWN5c0keSJkr6RtLRZa3Pe8+TIP+XZeQ1qrtpOrdRXeYvWl6kzIJFyznl6ucAqF61EsceugcrfltD/sJlTJo+j9n5SwAY+tEkuu7aihf5In07kAa5uXnMmzd303R+/jyaNm26eZm5c8nLy2PDhg2sWL6cevXqkZu3+WebNCn62fKuaW4u+TH7OD9/Ho0bNylapmlQJjc3PD4rllO3Xj3GjRvDkLff5OYb+rJ8+TKysrKoXKUKF1x0Sbp3I2mS9UdRUjbwBHA4MA8YK2momcWeML4BGGhmT0lqD4wAWpa2znQ0z3MkvRhm8MGSqsUulPSUpHGSJku6NWZ+F0mfS5okaYykmsU+113SF5IapGEfyjRu8hzaNm9Ii6b12S4nm5OO7MTwUd8UKVO/TvVNX4RrzjmSF4d8uemzdWpVpUHdoJZwUJedmPbjz+ndgTTo3KULM2b8wOxZs1i3bh2DBvSne4+eRcp079GTV19+EYA33xjMgQcfgiS69+jJoAH9Wbt2LbNnzWLGjB/o0rVrJnYjZTrt2YWZM2cwZ3ZwfN4cPJCjuh9TpEy37sfw+qsvAzDkrTc44MCDkcS773/MN1Nn8s3UmVx8yWVceXXfcp0wg56gOF9b1hWYYWY/mtk6oD/Qq1gZA2qF72sD8ylDOmqaOwHnmtloSS8Afy+2/Hoz+zX8i/A/SbsB04ABwMlmNlZSLWB14QckHQdcCRxtZkvJsIKCjfzfvQN558lLyM4SLw75kqk//syNF3dnwpSfGP7xtxzQeQdu69MTM/hswgyuuHsgABs3Gv946G1GPN0HSUyc+hMvvDk6w3uUfDk5OTz8yOMc0/1ICgoKOKv3ObTv0IHbbrmJTnt2pscxPel9zrmc0/sMOuzclrp16/Hyq/0BaN+hAyec9Fc67taenJwc/vXoE2RH+GmFf0ZOTg73PfgIJ/Q6moKCAk47szft2nfgrttvZo9OnTm6+zGccdY5XHTeWXTadSfq1q3L8y++lumwU0KIrKy463MNJI2LmX7WzJ6Nmc4F5sZMzwP2KraOW4D3JPUBqgOHlRlfSedSkkVSS+ATM2seTh8CXAbUAa42s3GSLgIuIEjgTYA+wGTgaTPbr9j6egPXACuBI8xsRSnbvSBcJ2xXY88qHc5K9q5tM5aOjdwp4khZs27burQpFepWzxlvZgn3UZQmp35rq3X0HXGVXfrKaWVuW9JJwJFmdl44fQbQ1cz6xJS5kiAXPihpH+B5YBcz21jSOtPRPC+elTdNS2oFXA0cama7AcOBKgQV79Ky+Y9ATWDHUjdo9qyZdTazzsqpujWxO+cyIIkdQfOAZjHTeWze/D4XGAhgZl8Q5KBST/ulI2k2D7M3wN+Az2KW1QJWAcslNQKOCudPA5pK6gIgqaakwlMJc4DjgZckdUh59M659EruOc2xwA6SWkmqBJwCDC1W5ifgUABJ7QiS5qLSVpiOpDkVOEvSN0A94KnCBWY2CZhI0Bx/ARgdzl8HnAw8JmkS8D7BjhR+bjpwGjBIUps07INzLo2SVdM0sw3ApcBIglw00MwmS7pNUmFP5FXA+WGueR3obWWct0xpR5CZzQZKukXhoJgyvUv57Fig+AWL/cIXZjaxlHU758oxkdwL181sBMFlRLHzbop5PwXYr/jnSuPXaTrnIidTt0jGw5Omcy5aFO07vjxpOucix5Omc84lwJOmc87FKdkdQcnmSdM5Fz3RzZmeNJ1zESMSufc87TxpOucix5vnzjmXiOjmTE+azrno8Zqmc87FKZPP/4mHJ03nXOR4R5BzziUiuhVNT5rOuejx5rlzzsXLB+xwzrn4CYhwzvSk6ZyLGu89d865hGT5IMTOORcnefPcOefiJrym6ZxzCfGapnPOJcA7gpxzLk6SN8+dcy4BfsmRc84lJMI505Omcy56vKbpnHPx8us0nXMufsG959HNmp40nXOR473nzjmXgAhXNLf9pNmxXXNGf/V4psOIrK63fZDpECLt+uPaZTqEisfH03TOufj5eJrOOZcQv7jdOecS4h1BzjkXL79O0znn4hf16zSj+0R251yFJSmuV5zr6iZpuqQZkvqWUuavkqZImizptbLW5zVN51zkJKuiKSkbeAI4HJgHjJU01MymxJTZAfgHsJ+ZLZW0fVnr9Jqmcy5ykljT7ArMMLMfzWwd0B/oVazM+cATZrYUwMx+KWuFnjSdc5Eiiays+F5xyAXmxkzPC+fF2hHYUdJoSV9K6lbWCr157pyLnASa5w0kjYuZftbMno1dVQmfsWLTOcAOwEFAHvCppF3MbFlJG/Sk6ZyLnKz4s+ZiM+tcxvJ5QLOY6TxgfgllvjSz9cAsSdMJkujYEmOLNzLnnEsXKb5XHMYCO0hqJakScAowtFiZt4GDg+2qAUFz/cfSVug1TedcpCiJA3aY2QZJlwIjgWzgBTObLOk2YJyZDQ2XHSFpClAAXGNmS0pbZ6lJU1KtLQSz4s/shHPObUky76I0sxHAiGLzbop5b8CV4WuLyqppTiY4YRobfuG0Ac3jC9k55xJTLu89N7NmpS1zzrlUEaASO72jIa6OIEmnSPpn+D5P0p6pDcs5V5FlKb5XRmLbUgFJjxP0LJ0RzvodeDqVQTnnKrA47wbK1KAe8fSe72tmnSRNBDCzX8Oue+ecS4kID3IUV9JcLymL8Cp6SfWBjSmNyjlXYQnIjnBHUDznNJ8A3gAaSroV+Ay4N6VROecqtHLdPDezlySNBw4LZ51kZt+lNiznXEWVwN0+GRHvHUHZwHqCJrrfeumcS6kE7j1Pu3h6z68HXgeaEtzs/pqkf6Q6MOdcxaU4X5kQT03zdGBPM/sdQNKdwHjg7lQG5pyruKL8jKB4kuacYuVyKGMEEOec2xqSIt17XtaAHQ8TnMP8HZgsaWQ4fQRBD7pzzqVEhCuaZdY0C3vIJwPDY+Z/mbpwnHOunDbPzez5dAbinHMQdPBEuHW+5XOaktoAdwLtgSqF881sxxTGVe68N/K/XH3l5RQUFND7nPO45tqij1deu3Yt5559JhMnjKdevfq88toAWrRsCcD9995Nv/88T3Z2Ng8+/CiHH3FkBvYg9fZrW5/rjt6RLIk3J+TzwqdzNitzRIftufjg1hjw/c+/0Xfwd3RpVZdruv3xdWvVoBrXDvqOj6YtSmP0qff16I948YGb2VhQwCHH/Y1eZ19aZPn7g1/mvYH9yMrKpkq16px/w73ktd6RGd9N5N93XAeAmXHihVfS9ZCjMrELSVMua5ox+gF3AA8ARwFn47dRFlFQUMAVl13C8HffJzcvj7/s3YUePXrSrn37TWX6vfA8devUZfK0GQwc0J/r/3kdr7w2gKlTpjBoQH8mTJrMgvnzObrbYXw75Xuys7MzuEfJlyX4Z4+duODFiSxcsYbXL+zKqGmL+XHRqk1lmteryrkHtOLM58axcs0G6lXfDoCxs5by16e+AqBW1RyGX74fX8wsdWDtcmljQQEv3HsD1z/5GvUbNeGfp3dnzwOPIK/1H38s9ut2LIefGIybM+7j93j5wVv5xxOv0qzNztz1ygiyc3JYumgh151yBHsecDjZOeXzwQwSZEc4acZzoXo1MxsJYGYzzewGwudpuMDYMWNo06YtrVq3plKlSpx08ikMe2dIkTLD3hnCaWecBcDxJ5zIqA//h5kx7J0hnHTyKVSuXJmWrVrRpk1bxo4Zk4ndSKld8mrz06+ryV+6mg0Fxn+/XcjBOzcsUuaEzrkM+GouK9dsAODXVes3W8/h7Rvx2Q+LWbN+2/q7PeO7r2mc15JGeS3I2a4S+x7Zi3Gj3itSplqNmpver139+6bekspVq25KkOvXrY10LS1eSXxGUNLF86dorYKfwkxJFwH5wPapDat8mT8/n7y8P8Zszs3NY8yYrzYv0ywok5OTQ63atVmyZAn5+fnstdfeRT47f35+egJPo0Y1K7Nw+ZpN0wtXrGHXvNpFyrSoXw2AF8/rTLbEUx/9yOgZRWuUR+3aiJc+/yn1AafZr4sWUL9xk03T9bZvzIzvJm5WbuSAfgx/9d9sWL+OG58ZsGn+D99O4Jlbr2bRgnlccvsj5baWWSjKiT+emub/ATWAy4D9gPOBc7Z2w5J+K2X+RZLODN/3ltR0a7eVasEjRooq/kMvtUwcn90mlLBLxXc9O0s0r1eNc18Yz3WDvuWWXu2oWeWPX/4GNSrRtlENPp+xbTXNgc2fxE3J34MjT+7No0NHc+pl/+St5x7dNH+HXTvxwOAPuevl4Qz5z+OsW7tms8+WJ1GuaW4xaZrZV2a20sx+MrMzzKynmY1OVUBm9rSZvRRO9ia4fTPScnPzmDdv7qbp/Px5NG3adPMyc4MyGzZsYMXy5dSrV4/cvM0/26RJ5Hc5YQtXrKVR7U39iDSqVYVFK9duVuajaYvYsNHIX7aG2Ut+p3m9apuWH7lLIz6c+gsbNpaQYcq5ets3YcnPCzZN//rLz9Rt2LjU8vse2Yuxo0ZuNj+39Q5UrlqNuTOnpyTOdBAiS/G9MqHUpCnpLUlvlvba0oolXSvpsvD9w5I+DN8fKumV8P2dkiZJ+lJSo3DeLZKulnQi0Bl4VdLXkqpK2lPSx5LGSxopqUlp20+nzl26MGPGD8yeNYt169YxaEB/uvfoWaRM9x49efXlFwF4843BHHjwIUiie4+eDBrQn7Vr1zJ71ixmzPiBLl27ZmI3Umpy/gpa1KtKbp0q5GSLbrs2YlSx3u+Ppi6ia6u6ANSpth0t6ldj3tLVm5YftWtj3v12YVrjTpc2HXbn57mz+CX/JzasX8fnI4ew54GHFymz4Kc/bsSb+On/aNKsFQC/5P9EwYbgPPCi+fNYMPtHGjYpx4/4irOWGcVzmo9v5bo/Aa4CHiVIfpUlbQf8BfgUOA340syul3QfQbP/jsIPm9ng8HnFV5vZuPCzjwG9zGyRpJMJLoXa7FSBpAuACwCaNU/9QzNzcnJ4+JHHOab7kRQUFHBW73No36EDt91yE5327EyPY3rS+5xzOaf3GXTYuS1169bj5Vf7A9C+QwdOOOmvdNytPTk5Ofzr0Se2uZ5zgIKNxl3Dp/PUmR3JzhJvT5jPzEWr+PshrZmSv4JR0xczesYS9mlbj7cu3ZuNBg+N/IHlq4POoKZ1qtCodmXGzV6a4T1JjeycHM6+7nbuuuQ0Nm7cyME9T6ZZm50Y+NT9tG6/O50PPIKRA/rx3VefkZ2TQ/Vatbn4tocBmDZxDEP7PUl2Tg7KyuKcf9xJrbr1MrxHWyfKvecq6VxbUlYcJLnpwO7AWwR3FvUHbic4PzoRqGJmFibAw83sPEm3AL+Z2QOSRvFH0twF+Jw/7nvPBhaY2RFlxbHnnp1t9Ffjkr+D24iut32Q6RAi7frj2mU6hMg7pVPeeDPrnKz1NWq7i538wOC4yj52XLukbjseKetiM7P1kmYTXNf5OfANwaVKbYCpwHr7I2MXxBGLgMlmtk9qInbORUWU7whK9YDCnwBXh/9/ClwEfG3xV29XAoUXp00neOTGPhDUZCV1SHK8zrkIKNeP8C0kqfKfWP+nQBPgCzNbCKwJ58WrH/C0pK8JmuMnAvdKmgR8Dez7J2JyzkVY0MlTjp8RJKkr8DxQG2guaXfgPDPrs6XPmtn/gO1ipneMeV8j5v1gYHD4/paY+W8QPNSt0NfAAVvarnOufMuO8EN14gntUaAHsATAzCbht1E651IkGOUoutdpxtMRlGVmc4pVhQtSFI9zzkX66Y3xJM25YRPdJGUDfYDvUxuWc64ii/BlmnElzYsJmujNgYXAB+E855xLOmWw6R2PLSZNM/sFOCUNsTjnHFDOa5qS/k0JY7CY2QUpicg5V6EJyInw1e3xNM9j77OrAhwHzC2lrHPObbVyXdM0swGx05JeBt5PWUTOuYotg3f7xOPP3HveCmiR7ECcc66QShq1OiK2eDmUpKWSfg1fywhqmf9MfWjOuYqo8BG+ybr3XFI3SdMlzZDUt4xyJ0oySWWOmlRmTTN8NtDuBM8FAtiYwGAbzjn3p2QnqX0eXlv+BHA4MA8YK2momU0pVq4mwZCVX22+lqLKrGmGCfItMysIX54wnXMpleSaZldghpn9aGbrCMb07VVCuduB+wgGFSpTPHcrjZHUKa7wnHNuayX2uIsGksbFvIpfCplL0at95oXz/tic1BFoZmbD4gmv1Oa5pBwz20DweIrzJc0EVgW7hJmZJ1LnXEokcEfQ4i2M3F7Sija1mCVlAQ8TPMQxLmWd0xwDdAKOjXdlzjm3tQqb50kyD4h9ylweMD9muiawCzAqHJSoMTBUUk8zK/E5OWUlTQGY2cytidg55xKVxIvbxwI7SGpF0KF9CnBq4UIzWw40+GO7fzyXrLQVlpU0G0q6srSFZvZQ/HE751x8hJL2NEoz2xA+1XYkwdMfXjCzyZJuA8aZ2dBE11lW0swGalDyOQHnnEuNJN8RZGYjgBHF5t1UStmDtrS+spLmAjO7LaHonHMuCcrr0HDRjdo5t80S5XfAjkPTFoVzzsUolzVNM/s1nYE451yhCOfMPzXKkXPOpYxE0nrPU8GTpnMucqKbMj1pOucipvC551HlSdM5FznRTZmeNJ1zERThiqYnTedctCTzNspU8KTpnIscedJ0zrn4RTdlVoCkuXp9AZPnrch0GJH1XO8umQ4h0m7/4PtMh1DxyGuazjkXNxHfc3gyxZOmcy5yvKbpnHMJSOZ4msnmSdM5FylB8zy6WdOTpnMuciLcOvek6ZyLGiGvaTrnXPy8pumcc3Hy8TSdcy5BEc6ZnjSdc9Hj5zSdcy5OwSDEmY6idJ40nXOR4zVN55xLgJ/TdM65OAnvPXfOuQT4xe3OORc/efPcOecSEuGc6UnTORct/txz55xLUIRzpidN51z0eEeQc84lwGuazjmXgAjnTE+azrkIinDWjPKTMp1zFZAovLx9y//iWp/UTdJ0STMk9S1h+ZWSpkj6RtL/JLUQ5Fi2AAAS50lEQVQoa32eNJ1z0aJglKN4XltclZQNPAEcBbQH/iapfbFiE4HOZrYbMBi4r6x1etJ0zkWP4nxtWVdghpn9aGbrgP5Ar9gCZvaRmf0eTn4J5JW1Qk+azrmIibdxLoAGksbFvC4otrJcYG7M9LxwXmnOBd4tKzrvCHLORU4ClxwtNrPOZa2qhHlW8jZ1OtAZOLCsDXrSTJLPP/6AB269jo0bCzj25DPpffGVRZa/8tzjDBnwEtnZOdStX5+b7n2CJnnNAehz1vF8O3Ece3TZm389PzAT4afFFx9/wEO392VjQQE9Tz6Tsy76vyLLX3v+cYYMfJmc7Gzq1GvADfc+TpPc5nw/5RvuvekqVv22kuysLHr//WoO73F8hvYidTrl1eL8fZuTJfH+tEUMnvRzkeWH7lifs/dqxpJV6wEYPnkh701fzK5NanLePs03lcurU4X7/zeTL+csS2v8yRJ/yzsu84BmMdN5wPzNtikdBlwPHGhma8taoSfNJCgoKODem67iiZffplHjXM7sdTAHHHY0rXfYeVOZnTvsxolDR1GlajUGv/Icj95zE3c/3g+AMy64jDWrV/Pm6//J0B6kXkFBAfffcjWPvfg22zduSu/jDmb/Q48qcox2bL8bL779EVWqVuONV5/n8Xtu5s7H/kOVqtW4+f6nad6qDYsWLuCsXgex9wGHULNWnQzuUXJlCS76SwtuHP49S1at46Hj2vPVnGXMXbamSLlPf/yVZ0b/VGTetwtWcvmbkwGoUTmbZ0/ejYnzVqQt9lRQ8q5uHwvsIKkVkA+cApxabFsdgWeAbmb2y5ZW6Oc0k2DypPE0a9GavOat2K5SJY445ng+fn94kTKd9zmAKlWrAbBLxy4s/PmPP3Zd9zuIajVqpDXmdJsyaTx5LVqT27wl21WqxOE9TuCTD0YUKVPkGO3RmV/CY9S8VVuat2oDQMNGTahbvwFLlyxJ7w6k2A4Nq7Ng+VoWrlzLho3GJzN/Za+WdRNez36t6jF+7nLWFmxMQZTpI8X32hIz2wBcCowEpgIDzWyypNsk9QyL3Q/UAAZJ+lrS0LLW6TXNJPjl5/k0avLHueXtG+fy3dfjSi0/ZMDL7Hvg4ekILTJ+Wbig2DFqyuRJ40stP3TQK+xz4GGbzZ88aTwb1q8nr0WrlMSZKfWrV2LxqnWbppesWseO21ffrNy+rerSoXFN5i9fw3NfzC3yGYD929RjyLc/b/a58iaZ17ab2QhgRLF5N8W83/yLVgZPmslgm59XLq15MeKtAUz9diLP9h9R4vJtVknHqJSi774dHKOnXytaW1/8y8/cctWF3HT/U2RlbVuNpBJ7K4odsjFzlvHxjF/ZsNHo1q4hVxzUihuGT9+0vG7V7WhZryoT5pbvpnmyT2omW+S+eZLKXSLfvkkuCxfkb5r+5ed8GjZqvFm5rz77iBeeeICH/t2fSpUrpzPEjNu+cdNix2g+DRo12azcmNGj6PfkgzzwzOtFjtFvK1dw5Xl/5aIrb2DXjl3SEnM6LV61jgbVK22arl+9Er/+vr5ImZVrC9iwMcik701bRNuG1Yos/0ubenwxeykFJfyBKm+SeUdQsqU9aUq6UdI0Se9Lel3S1ZJGSbpL0sfA5ZKOkfSVpImSPpDUSFKWpB8kNQzXkxXeFtUg3ftQXPvdOjF39kzy585m/bp1vPfOmxxw2NFFykybPIm7rr+Ch/7dn3oNGmYo0sxpFx6j+eExen/YGxxw6FFFykyfPIl7briC+595vcgxWr9uHdddfDpHHXcKhx59bLpDT4sfFq2iae3KNKpZiZwscUCbeoyZs7RImbpVt9v0vmuLOsxdWrST6IA29fhkxq9piTeVRPLOaaZCWmt1kjoDJwAdw21PAApPbNUxswPDcnWBvc3MJJ0HXGtmV0l6BTgN+BdwGDDJzBaXsJ0LgAsAGjdtVnxx0uXk5HDNrQ/Q58zjKdhYQM+TTqfNju14+qE7abdrRw48/GgevftGVq9aRd9LzgKgUdM8Hn6uPwDnndSN2T9+z+pVqzh6n3bceM9jJZ7PK89ycnK4+ub7uaz3CWzcWMAxJ55O6x3b8czDwTE64LCjeeyem/h91Sr+2Sc4Ro2b5vHAs/35YMRbTBz7OcuX/crwN14D4Kb7nmTH9rtlcpeSaqPB06N/4tajdiIrCz6Yvpiflq7htD2b8sPi3xkzZxnH7NKIvVrUocCMlWs38MioWZs+v32NSjSsUYnvFqzM4F4kT5SHhpOlsSov6QqgrpndHE4/RHDNVA/gZjP7OJy/K/Ag0ASoBMwys26SmgFDzKyTpP7AK2Y2rKxttt+to7089OPU7VQ5V7Cx/DflUun2D77PdAiRN+zCruO3cIF5QnbZvZMN/u9ncZVt17R6Urcdj3Q3z8v6+7Eq5v1jwONmtitwIVAFwMzmAgslHQLsxRZud3LOlU9Rbp6nO2l+BhwjqYqkGkD3UsrVJrgQFeCsYsueA14huN6qIDVhOucyKXnjdSRfWpOmmY0FhgKTgDeBccDyEoreQnCh6adA8XOWQwkuRN12b59xrqKLcNbMxOU9D5jZLZKqAZ8AD5rZv2MLmNkQYEgpn9+doANoWorjdM5lQOEgxFGViaT5bDgIaBXgRTObEO8Hw1GXLyboQXfObYviHGA4U9KeNM3s1C2XKvWz9wD3JDEc51wUedJ0zrl4Ze5un3h40nTORU6UL273pOmci5SIj9fhSdM5Fz1JHIQ46TxpOuciJ8I505Omcy56IpwzPWk65yImg/eVx8OTpnMugqKbNT1pOucipXAQ4qjypOmcixy/jdI55xLgdwQ551wiopszPWk656InwjnTk6ZzLloy+SiLeHjSdM5Fjt9G6ZxzCYhuyvSk6ZyLoAhXND1pOueixgchds65uEX9jqB0P/fcOefKNa9pOuciJyvCVU1Pms65aPHrNJ1zLn7+jCDnnEtUhLOmJ03nXOT4JUfOOZcAH0/TOecS4UnTOefiF+Xmucws0zGklKRFwJxMxxGjAbA400FEmB+fskXx+LQws4bJWpmk/xLsZzwWm1m3ZG07Htt80owaSePMrHOm44gqPz5l8+OTeX4bpXPOJcCTpnPOJcCTZvo9m+kAIs6PT9n8+GSYn9N0zrkEeE3TOecS4EnTOecS4EnTOecS4EnTOecS4EkzgyT58S+Dovzw64iQ1Dj8349VmvgvbQaZ2UYASSdLapTpeKJEkiy8tEPSIZLaS+qQ6biiQoF6wGhJB5lfBpM2njQzQFIHSaeG7wWcBqzPbFTREpMw+wC3A0cDr0raKaOBRYQFfgUeAg6UtJ23XNLDD3KaScoBugJHSTopTA5VgFqStstsdNER1qR2B44BDgDqA7OBHyr6cZLULvw/G/gU2B3IMrON3kxPPR8aLo0kZZnZBknDgQLgCEk1gCnASqCwuV7XzJZmMNSMkFTFzNaEk1nAUuBr4P8IEsMJYWI4VtInZha10X5SKqxJCnhG0vfAEuA2YAVwK9DXm+mp50kzTcJzdBvDyerAYIJfgB5Ad2BvYLmk1UA1SceY2drMRJt+kioDp0gaDxwFNAGuAvYHcs2seViuN3A68HGGQs2kOmb2q6SDgTbAucDrBEPFdZaUY2YbMhphBeBJM01iztFdCpwNHAgMDxevAL4HniCobdapYAmzBTCf4DiMCP8/MaxVXgLcIakf8ANwHHC2mS3JVLyZIOkooK+kT4GFZvYYcJ2k/YFmBOc2LwCezGCYFYKf00wjSd0JEuYJZvZb2LwcBowCOgLdzOx3M5ufwTDTKrxk5mSgMsH5ueHAcqCmpKpmNoGgo2wasAg4zcy+zVS8mSDpQOABgtMUNYELJD0IYGafmtlrBB1le3hnUOr5AU6vysBAM5stqWrYZF9CkCiGAKMzG176mdnPwONAK+Acgib5A8C9BB1AAK2Bh8zsWTObnpFAM0RSHtAQOIFgNPN9CY5RR0n3xBTdBdgNqNCdZOngSTNFSunFXAucL2knM1ttZibpAmBvM3vdzBakOcyMia0RmdnvBL/wbYAzgLcIhkC7RtK/gPcIznFWKJIOAa4FxgPzCJrffzez94AFwL6FPekEpzTOqkindTLFz2mmQLELs08Acgk6Lv4LPAw8FTavmgB/B07NVKyZEnNh/87ADDN7VdIvwPHAxWb2pKRZQHtgHzOL0nOeUi48LscCr5vZLEm1CGqR9SXtDVQFzjWzHwDM7O3MRVuxeNJMgWIXZp8CDAVeAR4hSJyrgN7AGoJzdFMyE2n6SToMqGVmb0q6DLgC+EjSVILODAE9JV0NPGtmX2Yw3IyQ1JrglEVVYGj4R3hFeKnaFQTN9TsLE2bsH2mXep40k0hSW2CZmS2W1Ak4BDiYoFkFcBDBMX/WzPqF121uLHlt26xKwGBJVwItCS4vahP+fwNwB0GN6iAgOzMhZo6k3QhaHh8C+wB7Ad8S9Jg/K+kNADNbUpgsPWGml4/cngTh+ctKwJvAZIJawPKwZ7gjcJWZHSbpKoKawg3AqxXtmrrCPxKSbiXozHjVzC6UVA3oQtDZsRroC1QNz3VWGJJ6AJcCdYBZBDc9HAyMBF6uSFdVRJl3BCVHVngC/u8Etaa+khqEPcONgV/CcrMILqv5b0VKmLGdYpLqEjTBBwLnSuoVJscvCE5jZAP1K2DCbETwx+JyM9ub4EqKbIJOsF4Ex6pSBkN0Ia9pJlF4S2RDgp7f94DngBoEd/8sIrgI+SQz+z5jQWaQpPZmNkXS+QQ1ymXAawQdGoPCe8pzzGx1RgPNgPCPyXDgWjP7LDwWTwG1gM+BL8zsq0zG6AJe09wKkvaVdEr4/hKC2kEfgtvaTiS4kH0NQbNzMHB8BU6Y+wDvSrqQoLbdB1hH0PwcENY411fEhAkQjjUwGDhY0i5mth7oT3AOvB0wIZPxuT94R9DWqQvcHV4r15LgFr+2wE4ENc4eQCPgVjPrl6EYMy5sVs4F8oELCa4x/Jzg8qv9CS5iX5SxAKNjIHARcL+kCQSXHJ0J3EJw6dWkzIXmCnlNcyuY2XCCnvETCJqVPxJcj/kBwRf8cYJrNGtkLMgMk7QvcD1Qm+B2yHlAPYJOjnbAZWb2WUW706ckZjYPuI/g0rQVBJerbQe0AH7OYGguhp/TTAJJvYB+wEVmNiCcNwR4xMw+zGRsmSapGXAkQQ3qCYKrDBaF12meC4wys5mZjDGqwtGM7gYuNDOvZUaEJ80kCS8XeRR4GRhDcL3hiZ4QAuGAwvcQDDjRwMx2znBIkSepCVCpot0NFXWeNJNI0rHAG8AgggFhZ2c2omiRtD1wKHA5cIofH1ceedJMsnAYr9leOyidpO3C3mHnyh1Pms45lwDvPXfOuQR40nTOuQR40nTOuQR40nTOuQR40nTOuQR40qzgJBVI+lrSd5IGhWNb/tl1HSRpWPi+p6S+ZZStI+nvf2Ibt4Sjusc1v1iZfpJOTGBbLSV9l2iMbtvmSdOtNrM9zGwXglGHLopdqEDC3xMzG2pm95RRpA7B+KPOlSueNF2sT4G2YQ1rqqQnCYYkaybpCElfSJoQ1khrAEjqJmmapM8IHopGOL+3pMfD940kvSVpUvjal+CWyjZhLff+sNw1ksZK+iYc3b1wXddLmi7pA4IRpMok6fxwPZMkvVGs9nyYpE8lfR/e+oqkbEn3x2z7wq09kG7b5UnTASAph+A5Pd+Gs3YCXjKzjgQPgrsBOMzMOgHjgCslVQH+DRxDMMRb41JW/yjwsZntDnQieCRIX2BmWMu9RtIRwA5AV2APYE9JB0jak2C0n44ESblLHLvzppl1Cbc3FTg3ZllL4ECgO/B0uA/nAsvNrEu4/vMltYpjO64C8vE0XVVJX4fvPwWeB5oCc2KeBLk3wXiOo8MnV1QieDzFzsCsmKcivsIfD5GLdQjBuJCYWQGwPBypPNYR4WtiOF2DIInWBN4qfPyFpKFx7NMuku4gOAVQg+AZO4UGhg+z+0HSj+E+HAHsFnO+s3a47Qo5YLQrmydNt9rM9oidESbGVbGzgPfN7G/Fyu0BJOs+XAF3m9kzxbZxxZ/YRj/gWDObJKk3wZMtCxVfl4Xb7mNmsckVSS0T3K6rALx57uLxJbCfgkcUI6mapB2BaUArSW3Ccn8r5fP/Ay4OP5stqRawkqAWWWgkcE7MudLccFSkT4DjJFWVVJPgVMCW1AQWhM/ZOa3YspMkZYUxtwamh9u+OCyPpB0lVY9jO64C8pqm2yIzWxTW2F6XVDmcfYOZfS/pAmC4pMXAZ8AuJazicuDZcNDhAuBiM/tC0ujwkp53w/Oa7YAvwprub8DpZjZB0gDga2AOwSmELbkR+Cos/y1Fk/N0gtH1GxEMGr1G0nME5zonKNj4IoJHTTi3GR/lyDnnEuDNc+ecS4AnTeecS4AnTeecS4AnTeecS4AnTeecS4AnTeecS4AnTeecS8D/A4FCcichP/IYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35e8af8c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(c, classes=[\"black\", \"white\", \"gray\"], normalize=True,title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 15\n",
    "img = m[picIndex].reshape((120, 120))\n",
    "img[img==1] = 255\n",
    "img[img==2] = 128\n",
    "imgLabel = labels[picIndex].reshape((120,120))\n",
    "imgLabel[imgLabel==1] = 255\n",
    "imgLabel[imgLabel==2] = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f35e8a25668>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35e8a9ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.savefig(\"mask.png\", Image.fromarray(img.astype(np.uint8)))\n",
    "plt.imshow(img, cmap=plt.get_cmap('gray'), vmin=0, vmax=255)\n",
    "\n",
    "# plt.savefig(\"mask.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f35e8980470>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35e89fe438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imgLabel, cmap=plt.get_cmap('gray'), vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f35e91916d8>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function install_repl_displayhook.<locals>.post_execute at 0x7f35f0fba950> (for post_execute):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Floating point image RGB values must be in the 0..1 range.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpost_execute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mpost_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                     \u001b[0mdraw_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# IPython >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/_pylab_helpers.py\u001b[0m in \u001b[0;36mdraw_all\u001b[0;34m(cls, force)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf_mgr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mf_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_cursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;31m# if toolbar:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m#     toolbar.set_cursor(cursors.WAIT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# if toolbar:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1299\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2435\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2437\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 566\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    791\u001b[0m         return self._make_image(\n\u001b[1;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m             unsampled=unsampled)\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# (of int or float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;31m# or an RGBA array of re-sampled input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0;31m# output is now a correctly sized RGBA array of uint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/cm.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(self, x, alpha, bytes, norm)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'f'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                         raise ValueError(\"Floating point image RGB values \"\n\u001b[0m\u001b[1;32m    258\u001b[0m                                          \"must be in the 0..1 range.\")\n\u001b[1;32m    259\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Floating point image RGB values must be in the 0..1 range."
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Floating point image RGB values must be in the 0..1 range.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2214\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m                     \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2216\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2217\u001b[0m                 \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m                 \u001b[0mbbox_inches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0moriginal_dpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;31m# if toolbar:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m#     toolbar.set_cursor(cursors.WAIT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# if toolbar:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1299\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2435\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2437\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 566\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    791\u001b[0m         return self._make_image(\n\u001b[1;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m             unsampled=unsampled)\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# (of int or float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;31m# or an RGBA array of re-sampled input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0;31m# output is now a correctly sized RGBA array of uint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/cm.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(self, x, alpha, bytes, norm)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'f'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                         raise ValueError(\"Floating point image RGB values \"\n\u001b[0m\u001b[1;32m    258\u001b[0m                                          \"must be in the 0..1 range.\")\n\u001b[1;32m    259\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Floating point image RGB values must be in the 0..1 range."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35e92070b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[picIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  ...\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]]\n",
      "\n",
      " [[0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  ...\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]]\n",
      "\n",
      " [[0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  ...\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  ...\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]]\n",
      "\n",
      " [[0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  ...\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]]\n",
      "\n",
      " [[0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  ...\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]\n",
      "  [0.57305594 0.57305594 0.57305594]]]\n"
     ]
    }
   ],
   "source": [
    "print (images[picIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(images[picIndex])? (<ipython-input-120-48575cfe0d6d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-120-48575cfe0d6d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print images[picIndex]\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(images[picIndex])?\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
